<p>本文介绍了 scrapy 的简单入门，使用 scrapy 爬取网站所有文章，文章结构如下: <br /></p>
<ol>
  <li>分析网页结构 <br /></li>
  <li>使用 css selector 的方法提取元素<br /></li>
  <li>开始 scrapy 工程 <br /></li>
  <li>获取所有文章url，爬取文章数据 <br /></li>
  <li>下载图片 <br /></li>
  <li>使用 item 和 itemloader <br /></li>
  <li>将数据导出到 json 格式的文件中 <br /></li>
  <li>将数据保存到 mysql 数据库中</li>
</ol>

<p>scrapy 是一个用 python 语言编写的，为了爬取网站数据，提取结构性数据而编写的应用框架。</p>
<h1 id="环境">环境</h1>
<p>本文使用的环境：<br />
python 3.5.2 <br />
pip 9.0.1 <br />
操作系统： Ubuntu 16.04</p>
<h2 id="pythton-环境搭建">pythton 环境搭建</h2>
<p>在官网下载 Ubuntu 环境下的python3.5的安装包，安装，安装完成后，检查一下 python 的安装情况，一般pyhton安装的时候，pip 也是一起安装好的，如果没有安装完全，再将 pip 也一起安装好。</p>
<h2 id="虚拟环境搭建">虚拟环境搭建</h2>
<p>现在Ubuntu默认是安装 python2.7 的，避免两个环境之间切换的麻烦，我们安装 python 虚拟环境来解决这个问题。<br /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install virtualenv
pip install virtualwrapper
pip list # 查看已安装 virtualenv 能够通过根据不同的 python 版本创建对应不同版本的虚拟环境，virtualwrapper 能够方便的在不同的虚拟环境之间进行切换。安装完成之后，下面我们创建一个 python3.5.2 版本的虚拟环境

source /usr/local/bin/virtualwrapper.sh #这个与 windows 不一样，需要先执行一下脚本才能生效，大家可以打开这个文件看一下
# 创建一个名为 py3Scrapy 的虚拟环境
mkvirtualenv py3Scrapy -p /usr/bin/python3.5
# workon 查看创建好的虚拟环境，虚拟环境的保存路径可以通过 `VIRTUALENV_PYTHON` 来配置
workon
workon py3Scrapy # 进入选择的虚拟环境
</code></pre></div></div>

<p>如下图所示： <br />
<img src="https://github.com/small-cat/small-cat.github.io/raw/master/_pics/spider_on_jobble/1.png" alt="进入虚拟环境" /></p>

<p>python的版本也能查看得到，进入虚拟环境之后，在shell前面会出现虚拟环境的名称，退出虚拟环境</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>deactivate
</code></pre></div></div>

<p>好了，创建好环境之后，现在来开始我们的 scrapy 之旅吧。</p>

<h2 id="scrapy-环境搭建">scrapy 环境搭建</h2>
<p>scrapy 是基于 twisted 框架的，大家会发现，安装 scrapy 的时候，会需要安装很多包。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install scrapy
</code></pre></div></div>

<p>使用 pip 进行安装，方便，但是这种默认的安装方式，实在官网下载安装包来进行安装的，比较慢，大家可以<strong>使用豆瓣源来进行安装</strong></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install scrapy -i https://pypi.douban.com/simple
</code></pre></div></div>

<p>这种方式，下载会非常的快，安装其他的包都可以使用这种方法，但是，如果使用豆瓣源安装的时候，提示找不到符合版本的安装包，那就使用第一种办法进行下载安装，因为豆瓣源可能没有官网那么及早更新。</p>

<p>因为每个人的环境都可能存在差异，安装过程中会出现一些问题。当如果报错，twisted 安装失败的时候，建议从官网下载 twisted 安装包，自行进行安装，安装完成之后，再接着继续上面 scrapy 的安装，安装完成之后，检查一些安装结果</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy -h
</code></pre></div></div>

<h1 id="使用-scrapy-获取某一个文章的信息">使用 scrapy 获取某一个文章的信息</h1>
<p>好了，环境准备好之后，接下来我们来分析一下伯乐在线的文章网页结构</p>
<h2 id="分析伯乐在线某一篇文章的网页结构和url">分析伯乐在线某一篇文章的网页结构和url</h2>
<p>伯乐在线网站不需要我们先登录，然后才能访问其中的内容，所以不需要先模拟登录，直接就能访问网页。伯乐在线地址为 <code class="highlighter-rouge">https://www.jobbole.com</code>，这上面的文章质量还是不错的，大家有时间可以看看。</p>

<p>我们随便找一篇文章试图来分析一下，比如 <code class="highlighter-rouge">http://blog.jobbole.com/111469/</code>，F12进入浏览器调试窗口，从全文分析，比如我们想获取文章标题，文章内容，文章创建时间，点赞数，评论数，收藏数，文章所属类别标签，文章出处等信息 <br /></p>
<h2 id="使用-scrapy-shell-的方法获取解析网页数据">使用 scrapy shell 的方法获取解析网页数据</h2>
<p>打开文章链接，我们获取到的是一个html页面，那么如何获取上面所说的那些数据呢，本文通过 CSS 选择器来获取(不了解 CSS selector的小伙伴可以先去熟悉一下 <code class="highlighter-rouge">http://www.w3school.com.cn/cssref/css_selectors.asp</code>)。 scrape 为我们提供了一个 shell 的环境，可以方便我们进行调试和实验，验证我们的css 表达式能够成功获取所需要的值。下面启动 scrapy shell</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy shell "http://blog.jobbole.com/111469/"
</code></pre></div></div>

<p>scrapy 将会帮助我们将<code class="highlighter-rouge">http://blog.jobbole.com/111469/</code>这个链接的数据捕获，现在来获取一下文章标题，在浏览器中找到文章标题，<code class="highlighter-rouge">inspect element</code> 审查元素，如下图所示：<br />
<img src="https://github.com/small-cat/small-cat.github.io/raw/master/_pics/spider_on_jobble/2.png" alt="css get title" /><br />
文章标题为<strong>王垠：如何掌握所有的程序语言</strong>，从上图获知，这个位于一个 class 名为 <code class="highlighter-rouge">entry-header</code> 的 div 标签下的子标签 h1 中，那我们在 scrapy shell 通过 css 选择器来获取一下，如下图所示：<br />
<img src="https://github.com/small-cat/small-cat.github.io/raw/master/_pics/spider_on_jobble/3.png" alt="get title" /><br />
仔细查看上图，注意一些细节。通过 response.css 方法，返回的结果是一个 selector，不是字符串，在这个 selector 的基础上可以继续使用 css 选择器。通过 extract() 函数获取提取的标题内容，返回结果是一个 list，注意，这里是一个 list ，仍然不是字符串 str，使用 extract()[0] 返回列表中的第一个元素，即我们需要的标题。</p>

<p>但是，如果标题没有获取到，或者选择器返回的结果为空的话，使用 extract()[0] 就会出错，因为试图对一个空链表进行访问，这里使用 extract_first() 方法更加合适，可是使用一个默认值，当返回结果为空的时候，返回这个默认值</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>extract_first("")   # 默认值为 ""
</code></pre></div></div>

<p>此处仅仅是将 title 标题作为一个例子进行说明，其他的就不详细进行解释了，主要代码如下所示：</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">        <span class="n">title</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".entry-header h1::text"</span><span class="p">).</span><span class="nf">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">match_date</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="s2">"([0-9/]*).*"</span><span class="p">,</span>
                              <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".entry-meta-hide-on-mobile::text"</span><span class="p">).</span><span class="nf">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="nf">strip</span><span class="p">())</span>
        <span class="k">if</span> <span class="ss">match_date:
            </span><span class="n">create_date</span> <span class="o">=</span> <span class="n">match_date</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">votes_css</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".vote-post-up h10::text"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">()</span>
        <span class="k">if</span> <span class="ss">votes_css:
            </span><span class="n">vote_nums</span> <span class="o">=</span> <span class="n">int</span><span class="p">(</span><span class="n">votes_css</span><span class="p">)</span>
        <span class="ss">else:
            </span><span class="n">vote_nums</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">ma_fav_css</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="s2">".*?(</span><span class="se">\d</span><span class="s2">+).*"</span><span class="p">,</span>
                              <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".bookmark-btn::text"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">())</span>
        <span class="k">if</span> <span class="ss">ma_fav_css:
            </span><span class="n">fav_nums</span> <span class="o">=</span> <span class="n">int</span><span class="p">(</span><span class="n">ma_fav_css</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="ss">else:
            </span><span class="n">fav_nums</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">ma_comments_css</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="s2">".*?(</span><span class="se">\d</span><span class="s2">+).*"</span><span class="p">,</span>
                                   <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"a[href='#article-comment'] span::text"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">())</span>
        <span class="k">if</span> <span class="ss">ma_comments_css:
            </span><span class="n">comment_nums</span> <span class="o">=</span> <span class="n">int</span><span class="p">(</span><span class="n">ma_comments_css</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="ss">else:
            </span><span class="n">comment_nums</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">tag_lists_css</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".entry-meta-hide-on-mobile a::text"</span><span class="p">).</span><span class="nf">extract</span><span class="p">()</span>
        <span class="n">tag_lists_css</span> <span class="o">=</span> <span class="p">[</span><span class="n">ele</span> <span class="k">for</span> <span class="n">ele</span> <span class="k">in</span> <span class="n">tag_lists_css</span> <span class="k">if</span> <span class="n">not</span> <span class="n">ele</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">endswith</span><span class="p">(</span><span class="s1">'评论'</span><span class="p">)]</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="s1">','</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">tag_lists_css</span><span class="p">)</span>

        <span class="n">content</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".entry *::text"</span><span class="p">).</span><span class="nf">extract</span><span class="p">()</span></code></pre></figure>

<p>解释一下 create_date，通过获取到的值，存在其他非时间的数据，通过 re.match 使用正则表达式来提去时间。</p>

<p>好了，所有需要的值都提取成功后，下面通过 scrapy 框架来创建我们的爬虫项目。</p>
<h2 id="创建爬虫项目">创建爬虫项目</h2>
<p>开始我们的爬虫项目</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy startproject ArticleSpider
</code></pre></div></div>

<p>scrapy 会为我们创建一个名为 ArticleSpider 的项目 <br />
进入到 ArticleSpider 目录，使用基本模板创建</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy genspider jobbole blog.jobbole.com
</code></pre></div></div>

<p>创建完成之后，我们使用 pycharm 这个IDE打开我们创建的爬虫项目，目录结构如下所示： <br /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── ArticleSpider
│   ├── items.py
│   ├── middlewares.py
│   ├── pipelines.py
│   ├── __pycache__
│   ├── settings.py
│   ├── spiders
│   │   ├── __init__.py
│   │   ├── jobbole.py
│   │   └── __pycache__
└── scrapy.cfg
</code></pre></div></div>

<p>我们可以在 items.py 里面定义数据保存的格式，在 middlewares.py 定义中间件，在 piplines.py 里面处理数据，保存到文件或者数据库中等。在 jobbole.py 中对爬取的页面进行解析。</p>

<p>下面，我们首先需要做的，就是利用我们编写的 css 表达式，获取我们提取的文章的值。在 jobbole.py 中，我们看到</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">class</span> <span class="nc">JobboleSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="no">Spider</span><span class="p">):</span>
        <span class="nb">name</span> <span class="o">=</span> <span class="s1">'jobbole'</span>
        <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'blog.jobbole.com'</span><span class="p">]</span>
        <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'http://blog.jobbole.com/all-posts/'</span><span class="p">]</span>
    
        <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">pass</span></code></pre></figure>

<p>scrapy 为我们创建了一个 JobboleSpider 的类，name 是爬虫项目的名称，同时定义了域名以及爬取的入口链接。scrapy 初始化的时候，会初始化 <code class="highlighter-rouge">start_urls</code> 入口链接列表，然后通过 <code class="highlighter-rouge">start_requests</code> 返回 Request 对象进行下载，调用 parse 回调函数对页面进行解析，提取需要的值，返回 item。</p>

<p>所以，我们需要做的，就是将我们在上一小节编写的代码放在 parse 函数中，同时，将 start_urls 的值，改为上面我们在 scrapy shell 爬取的页面的地址<code class="highlighter-rouge">http://blog.jobbole.com/111469/</code>，因为我们这里还没有讲到通过 item 获取我们提取的值，此处你可以通过 print() 函数将值进行打印。在 shell 中启动爬虫（先进入我们的工程目录）</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy crawl jobbole
</code></pre></div></div>

<p>既然我们使用的了 pycharm 这个IDE，那么我们就不用 shell 来启动爬虫，在 ArticleSpider 目录下创建一个 main.py 文件</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from scrapy.cmdline import execute
import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
execute(["scrapy", "crawl", "jobbole"])
</code></pre></div></div>

<p>上面的代码，就是将当前项目路径加入到 path 中，然后通过调用scrapy 命令行来启动我们的工程。然后，通过设置断点调试，一步一步查看我们的提取的变量的值是否正确。</p>

<blockquote>
  <p>注意：启动之前，将 settings.py 中的 <code class="highlighter-rouge">ROBOTSTXT_OBEY</code> 这个参数设置为 False</p>
</blockquote>

<p>这样，我们就爬取到了伯乐在线的这一篇文章了。</p>
<h1 id="扩展爬取所有的文章">扩展，爬取所有的文章</h1>
<p>既然我们已经能够获取到某一篇文章的数据，那么下面就来获取所有文章的链接。</p>
<h2 id="扩展一获取所有-url-链接">扩展一：获取所有 url 链接</h2>
<p>伯乐在线所有文章链接的入口地址为 <code class="highlighter-rouge">http://blog.jobbole.com/all-posts/</code>，通过浏览器进入调试模式查看文章列表的链接，如下图所示<br />
<img src="https://github.com/small-cat/small-cat.github.io/raw/master/_pics/spider_on_jobble/4.png" alt="文章列表链接" /><br />
文章链接是在 id 为 archive 的 div 标签下的子 div 标签之下， class 为 post-thumb，这个下面的子标签 a 的 href 属性，仍使用上面说的 scrapy shell 的方法，如下图所示 <br />
<img src="https://github.com/small-cat/small-cat.github.io/raw/master/_pics/spider_on_jobble/5.png" alt="url list" /><br />
可以看出，获得了当前页面所有的文章的 url，这仅仅是当前页面的所有 url，我们还需要获取下一页的 url，然后通过下一页的 url 进入到下一页，获取下一页的所有文章的 url，依次类推，知道爬取完所有的文章 url。</p>

<p>在文章列表的最后，有翻页，分析如下<br />
<img src="https://github.com/small-cat/small-cat.github.io/raw/master/_pics/spider_on_jobble/6.png" alt="next url" /><br />
下一页是 class 为 next page-numbers 的 a 标签中，如下图 <br />
<img src="https://github.com/small-cat/small-cat.github.io/raw/master/_pics/spider_on_jobble/7.png" alt="get next url" /><br />
既然现在所有的 url 都能够获取到了，那么现在我们将 jobbole.py 中的 parse 函数修改一下</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">post_nodes</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"#archive .floated-thumb .post-thumb"</span><span class="p">)</span>    <span class="c1"># a selector, 可以在这个基础上继续做 selector</span>
        
        <span class="k">for</span> <span class="n">post_node</span> <span class="k">in</span> <span class="ss">post_nodes:
            </span><span class="n">post_url</span> <span class="o">=</span> <span class="n">post_node</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"a::attr(href)"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
            <span class="k">yield</span> <span class="no">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">parse</span><span class="p">.</span><span class="nf">urljoin</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">url</span><span class="p">,</span> <span class="n">post_url</span><span class="p">),</span>
                          <span class="n">callback</span><span class="o">=</span><span class="nb">self</span><span class="p">.</span><span class="nf">parse_detail</span><span class="p">)</span>

        <span class="c1"># 必须考虑到有前一页，当前页和下一页链接的影响，使用如下所示的方法</span>
        <span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"span.page-numbers.current+a::attr(href)"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
         <span class="k">if</span> <span class="ss">next_url:
             </span><span class="k">yield</span> <span class="no">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">parse</span><span class="p">.</span><span class="nf">urljoin</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">url</span><span class="p">,</span> <span class="n">next_url</span><span class="p">),</span> <span class="n">callback</span><span class="o">=</span><span class="nb">self</span><span class="p">.</span><span class="nf">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_detail</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="s2">"""作为回调函数，在上面调用"""</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".entry-header h1::text"</span><span class="p">).</span><span class="nf">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">match_date</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="s2">"([0-9/]*).*"</span><span class="p">,</span>
                              <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".entry-meta-hide-on-mobile::text"</span><span class="p">).</span><span class="nf">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="nf">strip</span><span class="p">())</span>
        <span class="k">if</span> <span class="ss">match_date:
            </span><span class="n">create_date</span> <span class="o">=</span> <span class="n">match_date</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">votes_css</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".vote-post-up h10::text"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">()</span>
        <span class="k">if</span> <span class="ss">votes_css:
            </span><span class="n">vote_nums</span> <span class="o">=</span> <span class="n">int</span><span class="p">(</span><span class="n">votes_css</span><span class="p">)</span>
        <span class="ss">else:
            </span><span class="n">vote_nums</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">ma_fav_css</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="s2">".*?(</span><span class="se">\d</span><span class="s2">+).*"</span><span class="p">,</span>
                              <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".bookmark-btn::text"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">())</span>
        <span class="k">if</span> <span class="ss">ma_fav_css:
            </span><span class="n">fav_nums</span> <span class="o">=</span> <span class="n">int</span><span class="p">(</span><span class="n">ma_fav_css</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="ss">else:
            </span><span class="n">fav_nums</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">ma_comments_css</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="s2">".*?(</span><span class="se">\d</span><span class="s2">+).*"</span><span class="p">,</span>
                                   <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"a[href='#article-comment'] span::text"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">())</span>
        <span class="k">if</span> <span class="ss">ma_comments_css:
            </span><span class="n">comment_nums</span> <span class="o">=</span> <span class="n">int</span><span class="p">(</span><span class="n">ma_comments_css</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="ss">else:
            </span><span class="n">comment_nums</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">tag_lists_css</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".entry-meta-hide-on-mobile a::text"</span><span class="p">).</span><span class="nf">extract</span><span class="p">()</span>
        <span class="n">tag_lists_css</span> <span class="o">=</span> <span class="p">[</span><span class="n">ele</span> <span class="k">for</span> <span class="n">ele</span> <span class="k">in</span> <span class="n">tag_lists_css</span> <span class="k">if</span> <span class="n">not</span> <span class="n">ele</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">endswith</span><span class="p">(</span><span class="s1">'评论'</span><span class="p">)]</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="s1">','</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">tag_lists_css</span><span class="p">)</span>

        <span class="c1"># cpyrights = response.css(".copyright-area").extract()</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">".entry *::text"</span><span class="p">).</span><span class="nf">extract</span><span class="p">()</span></code></pre></figure>

<ol>
  <li>获取文章列表页中的文章url，交给 scrapy 下载后并进行解析，即调用 parse 函数解析</li>
  <li>然后获取下一页的文章 url，按照1 2 循环</li>
</ol>

<p>对于 parse 函数，一般做三种事情 <br />
a. 解析返回的数据 response data <br />
b. 提取数据，生成 ITEM <br />
c. 生成需要进一步处理 URL 的 Request 对象 <br /></p>

<blockquote>
  <p>某些网站中，url 仅仅只是一个后缀，需要将当前页面的url+后缀进行拼接，使用的是 <code class="highlighter-rouge">parse.urljoin(base, url)</code>，如果 urljoin 中的 url 没有域名，将使用base进行拼接，如果有域名，将不会进行拼接,此函数在 python3 的 urllib 库中。Request(meta参数)：meta参数是一个字典{},作为回调函数的参数</p>
</blockquote>

<p>这样，我们就获得了所有的文章</p>
<h2 id="扩展二使用item并保存图片到本地">扩展二：使用item，并保存图片到本地</h2>
<p>上一小节提到了， parse 函数提取数据之后，生成 item，scrapy 会通过 http 将 item 传到 pipeline 进行处理，那么这一小节，我们使用 item 来接收 parse 提取的数据。在 items.py 文件中，定义一个我们自己的数据类 JobBoleArticleItem，并继承 scrapy.item 类</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">class</span> <span class="nc">JobBoleArticleItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="no">Item</span><span class="p">):</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>          <span class="c1"># Field()能够接收和传递任何类型的值,类似于字典的形式</span>
        <span class="n">create_date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>    <span class="c1"># 创建时间</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>            <span class="c1"># 文章路径</span>
        <span class="n">front_img_url_download</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>
        <span class="n">fav_nums</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>       <span class="c1"># 收藏数</span>
        <span class="n">comment_nums</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>   <span class="c1"># 评论数</span>
        <span class="n">vote_nums</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>      <span class="c1"># 点赞数</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>           <span class="c1"># 标签分类 label</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>        <span class="c1"># 文章内容</span>
        <span class="nb">object_id</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>      <span class="c1"># 文章内容的md5的哈希值，能够将长度不定的 url 转换成定长的序列</span></code></pre></figure>

<p>Field() 对象，能够接收和传递任何类型的值，看源代码，就能发现，Field() 类继承自 dict 对象，具有字典的所有属性。</p>

<p>注意，在上面定义的类中，我们增加了一个新的成员变量 <code class="highlighter-rouge">front_img_url_download</code>，这是保存的是文章列表中，每一个文章的图片链接。我们需要将这个图片下载到本地环境中。既然使用了 item 接收我们提取的数据，那么 parse 函数就需要做相应的改动</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">post_nodes</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"#archive .floated-thumb .post-thumb"</span><span class="p">)</span>    <span class="c1"># a selector, 可以在这个基础上继续做 selector</span>
        
        <span class="k">for</span> <span class="n">post_node</span> <span class="k">in</span> <span class="ss">post_nodes:
            </span><span class="n">post_url</span> <span class="o">=</span> <span class="n">post_node</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"a::attr(href)"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
            <span class="n">img_url</span> <span class="o">=</span> <span class="n">post_node</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"a img::attr(src)"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
            <span class="k">yield</span> <span class="no">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">parse</span><span class="p">.</span><span class="nf">urljoin</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">url</span><span class="p">,</span> <span class="n">post_url</span><span class="p">),</span>
                          <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s2">"front-image-url"</span><span class="ss">:img_url</span><span class="p">},</span> <span class="n">callback</span><span class="o">=</span><span class="nb">self</span><span class="p">.</span><span class="nf">parse_detail</span><span class="p">)</span>

        <span class="c1"># 必须考虑到有前一页，当前页和下一页链接的影响，使用如下所示的方法</span>
        <span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="s2">"span.page-numbers.current+a::attr(href)"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
         <span class="k">if</span> <span class="ss">next_url:
             </span><span class="k">yield</span> <span class="no">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">parse</span><span class="p">.</span><span class="nf">urljoin</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">url</span><span class="p">,</span> <span class="n">next_url</span><span class="p">),</span> <span class="n">callback</span><span class="o">=</span><span class="nb">self</span><span class="p">.</span><span class="nf">parse</span><span class="p">)</span></code></pre></figure>

<p>同时，解析函数 parse_detail 也需要修改，将数据保存到我们的item中，只需要添加下面的部分就可</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">        <span class="n">front_img_url</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">meta</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="s2">"front-img-url"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
        <span class="n">article_item</span> <span class="o">=</span> <span class="no">JobBoleArticleItem</span><span class="p">()</span> <span class="c1"># 实例化 item 对象</span>
        <span class="c1"># 赋值 item 对象</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"title"</span><span class="p">]</span> <span class="o">=</span> <span class="n">title</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"create_date"</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_date</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"url"</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">url</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"front_img_url_download"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">front_img_url</span><span class="p">]</span> <span class="c1"># 这里传递的需要是列表的形式，否则后面保存图片的时候，会出现类型错误，必须是可迭代对象</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"fav_nums"</span><span class="p">]</span> <span class="o">=</span> <span class="n">fav_nums</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"comment_nums"</span><span class="p">]</span> <span class="o">=</span> <span class="n">comment_nums</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"vote_nums"</span><span class="p">]</span> <span class="o">=</span> <span class="n">vote_nums</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"tags"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tags</span>
        <span class="c1"># article_item["cpyrights"] = cpyrights</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span> <span class="o">=</span> <span class="s1">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>      <span class="c1"># 取出的 content 是一个 list ,存入数据库的时候，需要转换成字符串</span>
        <span class="n">article_item</span><span class="p">[</span><span class="s2">"object_id"</span><span class="p">]</span> <span class="o">=</span> <span class="n">gen_md5</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">url</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">article_item</span></code></pre></figure>

<p>这里，parse 函数成功生成了我们定义的 item 对象，将数据传递到 pipeline。那么，图片链接已经获取到了，我们如下将图片下载下来呢。</p>

<blockquote>
  <p>解释一下上面代码中的 front-img-url，这个是在 parse 函数中作为参数 meta 传递给 Request() 函数，回调函数调用 parse_detail，返回的 response 对象中的 meta 成员，将包含这个元素， meta 就是一个字典， response.meta.get(“front-img-url”) 将获取到我们传递过来的图片url</p>
</blockquote>

<p>scrapy 提供了一个 ImagesPipeline 类，可直接用于图片操作，只需要我们在 settings.py 文件中进行配置即可。</p>

<p>在 settings.py 中，有一个配置参数为 <code class="highlighter-rouge">ITEM_PIPELINE</code>，这其实就是一个字典，当需要用到 pipeline 时，就需要在这个字典中进行配置，字典中存在的， scrapy 才会使用。字典中的 key 就是 pipeline 的类名，后面的数字表示优先级，数字越小表示越先调用，越大越靠后。既然我们现在需要使用到 scrapy 提供的图片下载功能，那么需要在这个字典中配置 ImagesPipeline</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ITEM_PIPELINES = {
   'scrapy.pipelines.images.ImagesPipeline': 1,
}
</code></pre></div></div>

<p>同时，还需要在 settings.py 中配置，item 中哪一个字段是图片 url，以及图片需要存放什么位置</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>IMAGES_URLS_FIELD = "front_img_url_download"     # ITEM 中的图片 URL，用于下载
PROJECT_IMAGE_PATH = os.path.abspath(os.path.dirname(__file__))   # 获取当前文件所在目录
IMAGES_STORE = os.path.join(PROJECT_IMAGE_PATH, "images")         # 下载图片的保存位置
</code></pre></div></div>

<p>这些参数，可以在 ImagesPipeline 类的源代码中查看到</p>

<blockquote>
  <p><strong>注意：</strong>上面配置好后，上面的代码是在工程路径下面创建一个 images 的目录，用于保存图片，运行 main.py，可能会出现如下错误: no module named PIL，这是因为图片操作需要 pillow 库，只需要安装即可
<code class="highlighter-rouge">pip install pillow</code>，快速安装，就按照我上面说的豆瓣源的方法。<br />
还可能出现”ValueError: Missing scheme in request url: h”的错误，这是因为图片操作，要求 <code class="highlighter-rouge">front_img_url_download</code> 的值为 list 或者可以迭代的对象，所以我们在 parse 函数中给 item 赋值的时候， <code class="highlighter-rouge">front_img_url_download</code> 就是赋值的 list 对象</p>
</blockquote>

<p>好了，这些注意了之后，应该能够下载图片了。</p>
<h2 id="扩展三使用-itemloader">扩展三：使用 itemloader</h2>
<p>相信大家已经发现，虽然使用了 item，但是使用 css selecotor，我们的 parse 函数显得很长，而且，当数据量越来越大之后，一大堆的 css 表达式是很难维护的。在加上正则表达式的提取，代码会显得很臃肿。这里，给大家推荐是用 itemloader。itemloader 可以看成是一个容器。</p>

<p>首先，在 items.py 中，我们需要定义一个继承自 ItemLoader 的类</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">class</span> <span class="nc">ArticleItemLoader</span><span class="p">(</span><span class="no">ItemLoader</span><span class="p">):</span>
        <span class="s2">"""
        自定义 ItemLoader, 就相当于一个容器
        """</span>
        <span class="c1"># 这里表示，输出获取的 ArticleItemLoader 提取到的值，都是 list 中的第一个值</span>
        <span class="c1"># 如果有的默认不是取第一个值，就在 Field() 中进行修改</span>
        <span class="n">default_output_processor</span> <span class="o">=</span> <span class="no">TakeFirst</span><span class="p">()</span></code></pre></figure>

<p>将默认输出函数定为 TakeFirst()，即去结果 list 中的第一个值，定义了 ItemLoader 类之后，需要修改 jobbole.py 中的 <code class="highlighter-rouge">parse_detail</code> 函数了，现在就不再直接使用 css selector 了，使用 itemloader 中的 css 进行数据提取，新的 <code class="highlighter-rouge">parse_detail</code> 如下所示：</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">def</span> <span class="nf">parse_detail</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">front_img_url</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">meta</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="s2">"front-image-url"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
        <span class="n">item_loader</span> <span class="o">=</span> <span class="no">ArticleItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="no">JobBoleArticleItem</span><span class="p">(),</span> <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
        <span class="n">article_item_loader</span> <span class="o">=</span> <span class="no">JobBoleArticleItem</span><span class="p">()</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_css</span><span class="p">(</span><span class="s2">"title"</span><span class="p">,</span> <span class="s2">".entry-header h1::text"</span><span class="p">)</span>  <span class="c1"># 通过 css 选择器获取值</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_value</span><span class="p">(</span><span class="s2">"url"</span><span class="p">,</span> <span class="n">response</span><span class="p">.</span><span class="nf">url</span><span class="p">)</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_css</span><span class="p">(</span><span class="s2">"create_date"</span><span class="p">,</span> <span class="s2">".entry-meta-hide-on-mobile::text"</span><span class="p">)</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_value</span><span class="p">(</span><span class="s2">"front_img_url_download"</span><span class="p">,</span> <span class="p">[</span><span class="n">front_img_url</span><span class="p">])</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_css</span><span class="p">(</span><span class="s2">"fav_nums"</span><span class="p">,</span> <span class="s2">".bookmark-btn::text"</span><span class="p">)</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_css</span><span class="p">(</span><span class="s2">"comment_nums"</span><span class="p">,</span> <span class="s2">"a[href='#article-comment'] span::text"</span><span class="p">)</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_css</span><span class="p">(</span><span class="s2">"vote_nums"</span><span class="p">,</span> <span class="s2">".vote-post-up h10::text"</span><span class="p">)</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_css</span><span class="p">(</span><span class="s2">"tags"</span><span class="p">,</span> <span class="s2">".entry-meta-hide-on-mobile a::text"</span><span class="p">)</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_css</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">".entry *::text"</span><span class="p">)</span>
        <span class="n">item_loader</span><span class="p">.</span><span class="nf">add_value</span><span class="p">(</span><span class="s2">"object_id"</span><span class="p">,</span> <span class="n">gen_md5</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">url</span><span class="p">))</span>
        <span class="c1"># item_loader.add_xpath()</span>
        <span class="c1"># item_loader.add_value()</span>
        <span class="n">article_item_loader</span> <span class="o">=</span> <span class="n">item_loader</span><span class="p">.</span><span class="nf">load_item</span><span class="p">()</span>
        <span class="k">yield</span> <span class="n">article_item_loader</span></code></pre></figure>

<p>这样，数据提取就全部交给了 itemloader 来执行了。代码整体都简洁和工整了很多。ItemLoader 有三个方法用于提取数据，分别是 <code class="highlighter-rouge">add_css()</code>, <code class="highlighter-rouge">add_xpath()</code>, <code class="highlighter-rouge">add_value()</code>，前两个分别是 css 选择器和 xpath 选择器，如果是值，就直接使用 <code class="highlighter-rouge">add_value()</code> 即可。</p>

<p>最后 <code class="highlighter-rouge">load_item()</code> 函数，将根据上面提供的规则进行数据解析，每一个解析的值都是以 list 结果的形式呈现，同时，将结果赋值 item。</p>

<p>但是，大家应该已经发现，之前我们只接使用 css selector 提取数据的时候，对于某些数据，需要使用正则表达式进行匹配才能获取所需的值，这里什么都没做，仅仅是通过 itemloader 提取了数据而已。所以，我们还需要重新定义我们的 item 类，这些操作在 item 中进行处理。修改 items.py 中的 JobBoleArticleItem 类，具体如下：</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">class</span> <span class="nc">JobBoleArticleItem</span><span class="p">(</span><span class="n">scrapy</span><span class="p">.</span><span class="nf">item</span><span class="p">):</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>
        <span class="n">create_date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">(</span>     <span class="c1"># 创建时间</span>
            <span class="n">input_processor</span> <span class="o">=</span> <span class="no">MapCompose</span><span class="p">(</span><span class="n">get_date</span><span class="p">),</span>
            <span class="n">output_processor</span> <span class="o">=</span> <span class="no">Join</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>            <span class="c1"># 文章路径</span>
        <span class="n">front_img_url_download</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">(</span>    <span class="c1"># 文章封面图片路径,用于下载，赋值时必须为数组形式</span>
            <span class="c1"># 默认 output_processor 是 TakeFirst()，这样返回的是一个字符串，不是 list，此处必须是 list</span>
            <span class="c1"># 修改 output_processor</span>
            <span class="n">output_processor</span> <span class="o">=</span> <span class="no">MapCompose</span><span class="p">(</span><span class="n">return_value</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">front_img_url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>
        <span class="n">fav_nums</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">(</span>        <span class="c1"># 收藏数</span>
            <span class="n">input_processor</span><span class="o">=</span><span class="no">MapCompose</span><span class="p">(</span><span class="n">get_nums</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">comment_nums</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">(</span>    <span class="c1"># 评论数</span>
            <span class="n">input_processor</span><span class="o">=</span><span class="no">MapCompose</span><span class="p">(</span><span class="n">get_nums</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">vote_nums</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">(</span>       <span class="c1"># 点赞数</span>
            <span class="n">input_processor</span><span class="o">=</span><span class="no">MapCompose</span><span class="p">(</span><span class="n">get_nums</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">(</span>           <span class="c1"># 标签分类 label</span>
            <span class="c1"># 本身就是一个list, 输出时，将 list 以 commas 逗号连接</span>
            <span class="n">input_processor</span> <span class="o">=</span> <span class="no">MapCompose</span><span class="p">(</span><span class="n">remove_comment_tag</span><span class="p">),</span>
            <span class="n">output_processor</span> <span class="o">=</span> <span class="no">Join</span><span class="p">(</span><span class="s2">","</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">(</span>        <span class="c1"># 文章内容</span>
            <span class="c1"># content 我们不是取最后一个，是全部都要，所以不用 TakeFirst()</span>
            <span class="n">output_processor</span><span class="o">=</span><span class="no">Join</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="nb">object_id</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="no">Field</span><span class="p">()</span>      <span class="c1"># 文章内容的md5的哈希值，能够将长度不定的 url 转换成定长的序列</span></code></pre></figure>

<p><code class="highlighter-rouge">input_processor</code> 对传入的值进行预处理， <code class="highlighter-rouge">output_processor</code> 对处理后的值按照规则进行处理和提取，比如 <code class="highlighter-rouge">TakeFirst()</code> 就是对处理的结果去第一个值。</p>

<p><code class="highlighter-rouge">input_processor = MapCompose(func1, func2, func3, ...)</code> 这行代码，说明的是， Item 传入的这个字段的值，将会分别调用 MapCompose 中的所有传入的方法进行逐个处理，这个方法也是可以是 lambda 的匿名函数。</p>

<p>因为上面定义 ArticleItemLoader 类的时候，使用了默认的 <code class="highlighter-rouge">default_output_processor</code>，如果不想使用默认的这个方法，就在 Field() 中，使用 <code class="highlighter-rouge">output_processor</code> 参数覆盖默认的方法，哪怕什么都不做，也不会使用默认方法获取数据了。对上面那些方法定义如下：</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">def</span> <span class="nf">get_nums</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="s2">"""
        通过正则表达式获取 评论数，点赞数和收藏数
        """</span>
        <span class="n">re_match</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="s2">".*?(</span><span class="se">\d</span><span class="s2">+).*"</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="ss">re_match:
            </span><span class="n">nums</span> <span class="o">=</span> <span class="p">(</span><span class="n">int</span><span class="p">)(</span><span class="n">re_match</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="ss">else:
            </span><span class="n">nums</span> <span class="o">=</span> <span class="mi">0</span>
    
        <span class="k">return</span> <span class="n">nums</span>
    
    
    <span class="k">def</span> <span class="nf">get_date</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="n">re_match</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="s2">"([0-9/]*).*?"</span><span class="p">,</span> <span class="n">value</span><span class="p">.</span><span class="nf">strip</span><span class="p">())</span>
        <span class="k">if</span> <span class="ss">re_match:
            </span><span class="n">create_date</span> <span class="o">=</span> <span class="n">re_match</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="ss">else:
            </span><span class="n">create_date</span> <span class="o">=</span> <span class="s2">""</span>
        <span class="k">return</span> <span class="n">create_date</span>
    
    <span class="k">def</span> <span class="nf">remove_comment_tag</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="s2">"""
        去掉 tag 中的 “评论” 标签
        """</span>
        <span class="k">if</span> <span class="s2">"评论"</span> <span class="k">in</span> <span class="ss">value:
            </span><span class="k">return</span> <span class="s2">""</span>
        <span class="ss">else:
            </span><span class="k">return</span> <span class="n">value</span>
    
    
    <span class="k">def</span> <span class="nf">return_value</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="s2">"""
        do nothing, 只是为了覆盖 ItemLoader 中的 default_processor
        """</span>
        <span class="k">return</span> <span class="n">value</span></code></pre></figure>

<p><strong>千万注意：</strong>这些方法，每一个最后，都必须有 return，否则程序到后面将获取不到这个字段的数据，再次访问这个字段的时候，就会报错。</p>
<h2 id="扩展四将数据导出到-json-文件中">扩展四：将数据导出到 json 文件中</h2>
<p>好了，既然已经将数据通过 ItemLoader 获取到了，那么我们现在就将数据从 pipeline 输出到 json 文件中。</p>

<p>将数据以 json 格式输出，可以通过 json 库的方法，也可以使用 scrapy.exporters 的方法。</p>
<h3 id="json-库">json 库</h3>
<p>我们已经知道，对数据的处理，scrapy 是在 pipeline 中进行的，所以，我们需要在 pipelines.py 中定义我们对数据的导出操作。创建一个新类</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">class</span> <span class="nc">JsonWithEncodingPipeline</span><span class="p">(</span><span class="n">object</span><span class="p">):</span>
        <span class="s2">"""
        处理 item 数据，保存为json格式的文件中
        """</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">):</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">file</span> <span class="o">=</span> <span class="n">codecs</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="s1">'article.json'</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span>
    
        <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
            <span class="n">lines</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">dict</span><span class="p">(</span><span class="n">item</span><span class="p">),</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="no">False</span><span class="p">)</span> <span class="o">+</span> <span class="s1">'\n'</span>   <span class="c1"># False，才能够在处理非acsii编码的时候，不会出错，尤其</span>
            <span class="c1">#中文</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">item</span>     <span class="c1"># 必须 return</span>
    
        <span class="k">def</span> <span class="nf">spider_close</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
            <span class="s2">"""
            把文件关闭
            """</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">file</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span></code></pre></figure>

<p><code class="highlighter-rouge">__init__()</code> 构造对象的时候，就打开文件，scrapy 会调用 <code class="highlighter-rouge">process_item()</code> 函数对数据进行处理，在这个函数中，将数据以 json 的格式写入文件中。操作完成之后，将文件关闭。思路很简单。</p>

<h3 id="scrapyexporters-的方式">scrapy.exporters 的方式</h3>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">class</span> <span class="nc">JsonExporterPipeline</span><span class="p">(</span><span class="n">object</span><span class="p">):</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">):</span>
            <span class="s2">"""
            先打开文件，传递一个文件
            """</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'articleexporter.json'</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span>
            <span class="c1">#调用 scrapy 提供的 JsonItemExporter导出json文件</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">exporter</span> <span class="o">=</span> <span class="no">JsonItemExporter</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">"utf-8"</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="no">False</span><span class="p">)</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">exporter</span><span class="p">.</span><span class="nf">start_exporting</span><span class="p">()</span>
    
        <span class="k">def</span> <span class="nf">spider_close</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">exporter</span><span class="p">.</span><span class="nf">finish_exporting</span><span class="p">()</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">file</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
    
        <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">exporter</span><span class="p">.</span><span class="nf">export_item</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">item</span></code></pre></figure>

<p>scrapy.exporters 提供了几种不同格式的文件支持，能够将数据输出到这些不同格式的文件中，查看 JsonItemExporter 源码即可获知</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>__all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',
           'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter',
           'JsonItemExporter', 'MarshalItemExporter']
</code></pre></div></div>

<p>这些就是 scrapy 支持的文件。方法名称都差不多，这算是 scrapy 运行 pipeline 的模式，只需要将逻辑处理放在 <code class="highlighter-rouge">process_item()</code>，scrapy 就会根据规则对数据进行处理。</p>

<p>当然，要想使我们写的数据操作有效，别忘记了，在 settings.py 中进行配置</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ITEM_PIPELINES = {

  'scrapy.pipelines.images.ImagesPipeline': 1,
  'ArticleSpider.pipelines.JsonWithEncodingPipeline': 2,
  'ArticleSpider.pipelines.JsonExporterPipeline':3,
}
</code></pre></div></div>

<h2 id="扩展五将数据存储到-mysql-数据库">扩展五：将数据存储到 MySQL 数据库</h2>
<p>前面介绍了将数据以 json 格式导出到文件，那么将数据保存到 MySQL 中，如何操作，相信大家已经差不多了然于胸了。这里也介绍两种方法，一种是通过 MySQLdb 的API来实现的数据库存取操作，这种方法简单，适合用与数据量不大的场合，如果数据量大，数据库操作的速度跟不上数据解析的速度，就会造成数据拥堵。那么使用第二种方法就更好，使用 twisted 框架提供的异步操作方法，不会造成拥堵，速度更快。</p>

<p>既然是入 MySQL 数据库，首先肯定是需要创建数据库表了。表结构如下图所示： <br />
<img src="https://github.com/small-cat/small-cat.github.io/raw/master/_pics/spider_on_jobble/8.png" alt="desc article" /> <br />
上图中有一个字段的值，我没有讲述怎么取，就是 <code class="highlighter-rouge">front_img_path</code> 这个值，大家在数据库入库的时候，直接用空置或者空字符串填充即可。这个字段是保存图片在本地保存的路径，这个需要在 ImagesPipe 的 <code class="highlighter-rouge">item_completed(self, results, item, info)</code> 方法中的 results 参数中获取。</p>

<p>好了，数据库表创建成功之后，下面就来将数据入库了。</p>

<h3 id="mysqldb-的方法入库">MySQLdb 的方法入库</h3>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">class</span> <span class="nc">MysqlPipeline</span><span class="p">(</span><span class="n">object</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">):</span>
            <span class="c1"># 连接数据库</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">conn</span> <span class="o">=</span> <span class="no">MySQLdb</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="s1">'192.168.0.101'</span><span class="p">,</span> <span class="s1">'spider'</span><span class="p">,</span> <span class="s1">'wuzhenyu'</span><span class="p">,</span> <span class="s1">'article_spider'</span><span class="p">,</span> <span class="n">charset</span><span class="o">=</span><span class="s2">"utf8"</span><span class="p">,</span> <span class="n">use_unicode</span><span class="o">=</span><span class="no">True</span><span class="p">)</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">cursor</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">conn</span><span class="p">.</span><span class="nf">cursor</span><span class="p">()</span>
    
        <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
            <span class="n">insert_sql</span> <span class="o">=</span> <span class="s2">"""
                insert into article(title, create_date, url, url_object_id, front_img_url, front_img_path, comment_nums, 
                fav_nums, vote_nums, tags, content) VALUES ('%s', '%s', '%s', '%s', '%s', '%s', %d, %d, %d, '%s', '%s')
            """</span> <span class="o">%</span> <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">"title"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"create_date"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"url"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"object_id"</span><span class="p">],</span><span class="n">item</span><span class="p">[</span><span class="s2">"front_img_url"</span><span class="p">],</span>
                   <span class="n">item</span><span class="p">[</span><span class="s2">"front_img_path"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"comment_nums"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"fav_nums"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"vote_nums"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"tags"</span><span class="p">],</span>
                   <span class="n">item</span><span class="p">[</span><span class="s2">"content"</span><span class="p">])</span>
    
            <span class="nb">self</span><span class="p">.</span><span class="nf">cursor</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="n">insert_sql</span><span class="p">)</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">conn</span><span class="p">.</span><span class="nf">commit</span><span class="p">()</span>
    
        <span class="k">def</span> <span class="nf">spider_close</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">cursor</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">conn</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span></code></pre></figure>

<p>如果对 API 想了解的更多，就去阅读 python MySQLdb 的相关API文档说明，当然，要想这个生效，首先得在 settings.py 文件中将这个 pipeline 类加入 ITEM_PIPELINE 字典中</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ITEM_PIPELINES = {

  'scrapy.pipelines.images.ImagesPipeline': 1,
  'ArticleSpider.pipelines.JsonWithEncodingPipeline': 2,
  'ArticleSpider.pipelines.JsonExporterPipeline':3,
  'ArticleSpider.pipelines.MysqlPipeline': 4,
}
</code></pre></div></div>

<h3 id="通过-twisted-框架提供的异步方法入库">通过 Twisted 框架提供的异步方法入库</h3>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">class</span> <span class="nc">MysqlTwistedPipeline</span><span class="p">(</span><span class="n">object</span><span class="p">):</span>
        <span class="s2">"""
        利用 Twisted API 实现异步入库 MySQL 的功能
        Twisted 提供的是一个异步的容器，MySQL 的操作还是使用的MySQLDB 的库
        """</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">dbpool</span><span class="p">):</span>
            <span class="nb">self</span><span class="p">.</span><span class="nf">dbpool</span> <span class="o">=</span> <span class="n">dbpool</span>
    
        <span class="vi">@classmethod</span>
        <span class="k">def</span> <span class="nf">from_settings</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
            <span class="s2">"""
            被 spider 调用，将 settings.py 传递进来，读取我们配置的参数
            模仿 images.py 源代码中的 from_settings 函数的写法
            """</span>
            <span class="c1"># 字典中的参数，要与 MySQLdb 中的connect 的参数相同</span>
            <span class="n">dbparams</span> <span class="o">=</span> <span class="n">dict</span><span class="p">(</span>
                <span class="n">host</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s2">"MYSQL_HOST"</span><span class="p">],</span>
                <span class="n">db</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s2">"MYSQL_DBNAME"</span><span class="p">],</span>
                <span class="n">user</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s2">"MYSQL_USER"</span><span class="p">],</span>
                <span class="n">passwd</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s2">"MYSQL_PASSWORD"</span><span class="p">],</span>
                <span class="n">charset</span> <span class="o">=</span> <span class="s2">"utf8"</span><span class="p">,</span>
                <span class="n">cursorclass</span> <span class="o">=</span> <span class="no">MySQLdb</span><span class="p">.</span><span class="nf">cursors</span><span class="o">.</span><span class="no">DictCursor</span><span class="p">,</span>
                <span class="n">use_unicode</span> <span class="o">=</span> <span class="no">True</span>
            <span class="p">)</span>
    
            <span class="c1"># twisted 中的 adbapi 能够将sql操作转变成异步操作</span>
            <span class="n">dbpool</span> <span class="o">=</span> <span class="n">adbapi</span><span class="o">.</span><span class="no">ConnectionPool</span><span class="p">(</span><span class="s2">"MySQLdb"</span><span class="p">,</span> <span class="o">**</span><span class="n">dbparams</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">dbpool</span><span class="p">)</span>
    
        <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
            <span class="s2">"""
            使用 twisted 将 mysql 操作编程异步执行
            """</span>
            <span class="n">query</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">dbpool</span><span class="p">.</span><span class="nf">runInteraction</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">do_insert</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
            <span class="n">query</span><span class="p">.</span><span class="nf">addErrback</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">handle_error</span><span class="p">)</span> <span class="c1"># handle exceptions</span>
    
        <span class="k">def</span> <span class="nf">handle_error</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">failure</span><span class="p">):</span>
            <span class="s2">"""
            处理异步操作的异常
            """</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">failure</span><span class="p">)</span>
    
        <span class="k">def</span> <span class="nf">do_insert</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">cursor</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="s2">"""
            执行具体的操作，能够自动 commit
            """</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">"create_date"</span><span class="p">])</span>
            <span class="n">insert_sql</span> <span class="o">=</span> <span class="s2">"""
                        insert into article(title, create_date, url, url_object_id, front_img_url, front_img_path, comment_nums, 
                        fav_nums, vote_nums, tags, content) VALUES ('%s', '%s', '%s', '%s', '%s', '%s', %d, %d, %d, '%s', '%s');
                    """</span> <span class="o">%</span> <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">"title"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"create_date"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"url"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"object_id"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"front_img_url"</span><span class="p">],</span>
                           <span class="n">item</span><span class="p">[</span><span class="s2">"front_img_path"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"comment_nums"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"fav_nums"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"vote_nums"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">"tags"</span><span class="p">],</span>
                           <span class="n">item</span><span class="p">[</span><span class="s2">"content"</span><span class="p">])</span>
    
            <span class="c1"># self.cursor.execute(insert_sql, (item["title"], item["create_date"], item["url"], item["object_id"],</span>
            <span class="c1">#                                 item["front_img_url"], item["front_img_path"], item["comment_nums"],</span>
            <span class="c1">#                                 item["fav_nums"], item["vote_nums"], item["tags"], item["content"]))</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">insert_sql</span><span class="p">)</span>
            <span class="n">cursor</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="n">insert_sql</span><span class="p">)</span></code></pre></figure>

<p>博主也是近一个多星期开始学习爬虫的 scrapy 框架，对 Twisted 框架也不怎么熟悉，上面的代码是一个例子，大家可以看下注释，等以后了解更多会补充更多相关知识。</p>

<p>需要提到的是，上面定义的 <code class="highlighter-rouge">from_settings(cls. settings)</code> 这个类方法， scrapy 会从 settings.py
文件中读取配置进行加载，这里将 MySQL 的一些配置信息放在了 settings.py 文件中，然后使用 <code class="highlighter-rouge">from_settings</code> 方法直接获取，在 settings.py 中需要添加如下代码：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># MySQL params
MYSQL_HOST = ""
MYSQL_DBNAME = "article_spider"
MYSQL_USER = "spider"
MYSQL_PASSWORD = ""
</code></pre></div></div>

<blockquote>
  <p>本篇文章，主要以 scrapy 框架爬取伯乐在线文章为例，简要介绍了 scrapy 爬取数据的一些方法，博主也是最近才开始学习爬虫，有不对的地方还请大家能够指正。</p>
</blockquote>
